# -*- coding: utf-8 -*-
"""
QA Script ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• '‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÉ‡∏à‡∏ß‡∏±‡∏¢‡πÄ‡∏£‡∏µ‡∏¢‡∏ô AI' (‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Google Colab)

‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô Google Colab:
1. ‡∏£‡∏±‡∏ô‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏ô‡∏µ‡πâ
2. ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
3. ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏Ç‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏∏‡∏ì Login ‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà Hugging Face
4. ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏õ‡∏∏‡πà‡∏° "Choose Files" ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∏‡∏ì‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå `test_dataset.jsonl`
5. ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏™‡∏£‡πá‡∏à ‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå‡∏à‡∏∞‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö
6. ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô Human Evaluation ‡πÉ‡∏ô‡∏ä‡πà‡∏≠‡∏á input ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡∏Ç‡∏∂‡πâ‡∏ô
7. ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÄ‡∏™‡∏£‡πá‡∏à ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå `qa_results.csv` ‡πÅ‡∏•‡∏∞‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
"""

# ==============================================================================
# 1. ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Libraries ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
# ==============================================================================
!pip install -U rouge_score
import torch
import pandas as pd
import json
import evaluate
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import os
import sys
from huggingface_hub import notebook_login
from peft import PeftModel, PeftConfig

# --- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏£‡∏±‡∏ô‡πÉ‡∏ô Google Colab ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà ---
IN_COLAB = 'google.colab' in sys.modules

# ==============================================================================
# 2. Configuration
# ==============================================================================
# ‚ö†Ô∏è ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ: ‡∏£‡∏∞‡∏ö‡∏∏ Path ‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì Fine-tune ‡∏ö‡∏ô Hugging Face Hub
MODEL_PATH = "Pathfinder9362/Student-Mind-Mate-AI-v2"
OUTPUT_FILE = "qa_results.csv"
OFFLOAD_DIR = "./offload_qa"
os.makedirs(OFFLOAD_DIR, exist_ok=True)

# ==============================================================================
# 3. Login ‡πÅ‡∏•‡∏∞ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Colab)
# ==============================================================================
if IN_COLAB:
    print("\nü§ó ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤ Login ‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà Hugging Face Hub:")
    notebook_login()

    from google.colab import files
    print("\nüìÇ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå `test_dataset.jsonl` ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì:")
    uploaded = files.upload()

    # **‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç:** ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô
    if not uploaded:
        sys.exit("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î! ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏±‡∏ô‡πÄ‡∏ã‡∏•‡∏•‡πå‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á")
    TEST_CASES_FILE = next(iter(uploaded))
    print(f"‚úÖ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå `{TEST_CASES_FILE}` ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
else:
    TEST_CASES_FILE = "test_dataset.jsonl" # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô‡πÅ‡∏ö‡∏ö Local

# ==============================================================================
# 4. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•
# ==============================================================================
def load_model(peft_model_id):
    """
    **‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç:** ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡πâ‡∏ß‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥
    ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î Base model ‡πÅ‡∏ö‡∏ö 4-bit ‡∏Å‡πà‡∏≠‡∏ô ‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏∂‡∏á‡∏™‡∏ß‡∏° Adapter ‡∏ó‡∏±‡∏ö
    """
    print(f"\nüîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏≤‡∏Å: {peft_model_id}...")
    try:
        # --- 1. ‡πÇ‡∏´‡∏•‡∏î Config ‡∏Ç‡∏≠‡∏á Adapter ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤ Base Model ---
        config = PeftConfig.from_pretrained(peft_model_id)
        base_model_name = config.base_model_name_or_path
        print(f"   - ‡∏û‡∏ö Base Model: {base_model_name}")

        # --- 2. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Quantization ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Base Model ---
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
            llm_int8_enable_fp32_cpu_offload=True # **‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç:** ‡πÄ‡∏û‡∏¥‡πà‡∏° flag ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ offload
        )

        # --- 3. ‡πÇ‡∏´‡∏•‡∏î Base Model (‡πÅ‡∏ö‡∏ö 4-bit) ‡πÅ‡∏•‡∏∞ Tokenizer ---
        print("   - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î Base Model (4-bit)...")
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True,
            offload_folder=OFFLOAD_DIR,
            low_cpu_mem_usage=True
        )
        tokenizer = AutoTokenizer.from_pretrained(base_model_name)
        tokenizer.pad_token = tokenizer.eos_token

        # --- 4. ‡πÇ‡∏´‡∏•‡∏î Adapter ‡πÅ‡∏•‡∏∞‡∏™‡∏ß‡∏°‡∏ó‡∏±‡∏ö Base Model ---
        print("   - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î Adapter (LoRA)...")
        model = PeftModel.from_pretrained(base_model, peft_model_id)

        print("‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
        return model, tokenizer
    except Exception as e:
        print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•: {e}")
        import traceback
        traceback.print_exc()
        return None, None

# ==============================================================================
# 5. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
# ==============================================================================
def generate_response(prompt, model, tokenizer):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡∏≤‡∏° Prompt ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î"""
    system_prompt = (
       "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ '‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÉ‡∏à‡∏ß‡∏±‡∏¢‡πÄ‡∏£‡∏µ‡∏¢‡∏ô AI' ‡πÄ‡∏õ‡πá‡∏ô AI Chatbot ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡∏≠‡∏Å‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à, ‡πÉ‡∏´‡πâ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÉ‡∏à, ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô ‡πÇ‡∏î‡∏¢‡∏°‡∏µ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏±‡∏Å‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:\n"
        "1. ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏£‡∏±‡∏ö‡∏ü‡∏±‡∏á‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ñ‡∏£‡∏µ‡∏¢‡∏î‡∏Ç‡∏≠‡∏á‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô/‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤\n"
        "2. ‡∏´‡∏≤‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô ‡πÉ‡∏´‡πâ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏ß‡∏•‡∏≤ ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô\n"
        "3. **‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î:** ‡∏´‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏£‡πâ‡∏≤‡∏¢‡∏£‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏ô‡∏ï‡∏£‡∏≤‡∏¢ ‡πÉ‡∏´‡πâ‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡πÅ‡∏•‡∏∞‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏ó‡∏±‡∏ô‡∏ó‡∏µ ‡πÇ‡∏î‡∏¢‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ö‡∏≠‡∏£‡πå‡πÇ‡∏ó‡∏£‡∏®‡∏±‡∏û‡∏ó‡πå ‡πÄ‡∏ä‡πà‡∏ô ‡∏™‡∏≤‡∏¢‡∏î‡πà‡∏ß‡∏ô‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡∏à‡∏¥‡∏ï 1323 ‡∏´‡∏£‡∏∑‡∏≠‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡∏ô‡∏±‡∏Å‡∏à‡∏¥‡∏ï‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤"
    )
    chat_prompt = (
        f"<|im_start|>system\n{system_prompt}<|im_end|>\n"
        f"<|im_start|>user\n{prompt}<|im_end|>\n"
        f"<|im_start|>assistant\n"
    )

    inputs = tokenizer(chat_prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(
        **inputs,
        max_new_tokens=256,
        temperature=0.7,
        top_p=0.9,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )
     # **‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç:** ‡∏ñ‡∏≠‡∏î‡∏£‡∏´‡∏±‡∏™‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡πÇ‡∏ó‡πÄ‡∏Ñ‡πá‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
    new_tokens = outputs[0, input_token_len:]
    response_only = tokenizer.decode(new_tokens, skip_special_tokens=True)

# ==============================================================================
# 6. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö
# ==============================================================================
def run_qa_test():
    """‡∏£‡∏±‡∏ô‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ QA ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"""
    model, tokenizer = load_model(MODEL_PATH)
    if not model or not tokenizer:
        return

    rouge_metric = evaluate.load('rouge')

    # **‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç:** ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå JSON ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢ object ‡πÉ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
    try:
        with open(TEST_CASES_FILE, 'r', encoding='utf-8') as f:
            file_content = f.read()
            # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà JSON object ‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ô‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏°‡∏µ newline
            corrected_content = file_content.replace('}{', '}\n{')
            test_cases = [json.loads(line) for line in corrected_content.strip().split('\n')]
    except json.JSONDecodeError as e:
        print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå JSON: {e}")
        print("   - ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå test_dataset.jsonl ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö JSON ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á")
        return


    results = []
    print("\n=============================================")
    print("üöÄ      ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö QA      üöÄ")
    print("=============================================\n")

    # **‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç:** ‡∏õ‡∏£‡∏±‡∏ö‡∏•‡∏π‡∏õ‡πÉ‡∏´‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å format 'messages'
    for i, case in enumerate(test_cases):
        try:
            messages = case['messages']
            prompt = next(m['content'] for m in messages if m['role'] == 'user')
            reference_answer = next(m['content'] for m in messages if m['role'] == 'assistant')
            case_id = i + 1
        except (KeyError, StopIteration, IndexError):
            print(f"‚ö†Ô∏è ‡∏Ç‡πâ‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ô‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà {i+1}: {case}")
            continue

        print(f"--- üß™ ‡πÄ‡∏Ñ‡∏™‡∏ó‡∏î‡∏™‡∏≠‡∏ö #{case_id} ---")
        print(f"üë§ User: {prompt}")
        print(f"üìö Reference: {reference_answer}")

        generated_answer = generate_response(prompt, model, tokenizer)
        print(f"ü§ñ AI: {generated_answer}")

        rouge_scores = rouge_metric.compute(
            predictions=[generated_answer],
            references=[reference_answer]
        )

        print("\n--- ‚öñÔ∏è Human Evaluation (‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô 1-5) ---")
        try:
            empathy_score = int(input("‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡∏≠‡∏Å‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à (Empathy): "))
            helpfulness_score = int(input("‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå (Helpfulness): "))
            safety_score = int(input("‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢ (Safety): "))
            persona_score = int(input("‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏á‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó (Persona): "))
        except ValueError:
            print("‚ö†Ô∏è ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏™‡πà‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç 1-5 ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô! ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏õ‡πá‡∏ô 0")
            empathy_score, helpfulness_score, safety_score, persona_score = 0, 0, 0, 0

        results.append({
            'id': case_id,
            'prompt': prompt,
            'reference_answer': reference_answer,
            'generated_answer': generated_answer,
            'rouge1': rouge_scores['rouge1'],
            'rouge2': rouge_scores['rouge2'],
            'rougeL': rouge_scores['rougeL'],
            'empathy_score': empathy_score,
            'helpfulness_score': helpfulness_score,
            'safety_score': safety_score,
            'persona_score': persona_score,
        })
        print("-" * 50 + "\n")

    df = pd.DataFrame(results)
    df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')

    if IN_COLAB:
        from google.colab import files
        print(f"\nüéâ ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô! ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ó‡∏µ‡πà {OUTPUT_FILE}")
        print("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå...")
        files.download(OUTPUT_FILE)
    else:
        print(f"üéâ ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô! ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ó‡∏µ‡πà {OUTPUT_FILE}")

if __name__ == "__main__":
    run_qa_test()




