# -*- coding: utf-8 -*-
"""
Standalone Model Debugging Script

‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• AI ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏¢‡∏Å‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Ç‡∏≠‡∏á Docker ‡∏´‡∏£‡∏∑‡∏≠ FastAPI

‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ:
1. ‡πÄ‡∏õ‡∏¥‡∏î Terminal ‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏ó‡∏µ‡πà Directory 'backend/'
2. (‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å) ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ: pip install -r requirements.txt
3. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Hugging Face Token: export HUGGING_FACE_TOKEN='hf_...' (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Mac/Linux)
   ‡∏´‡∏£‡∏∑‡∏≠ $env:HUGGING_FACE_TOKEN='hf_...' (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö PowerShell)
4. ‡∏£‡∏±‡∏ô‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå: python debug_model.py
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import os
import sys
from peft import PeftModel, PeftConfig

# ==============================================================================
# 1. Configuration
# ==============================================================================
# ‡∏≠‡πà‡∏≤‡∏ô‡∏Ñ‡πà‡∏≤ Model Path ‡∏à‡∏≤‡∏Å Environment Variable, ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
MODEL_PATH = os.getenv("MODEL_PATH", "Pathfinder9362/Student-Mind-Mate-AI-v2")
OFFLOAD_DIR = "./offload_debug"
os.makedirs(OFFLOAD_DIR, exist_ok=True)


# ==============================================================================
# 2. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• (Boilerplate)
# ==============================================================================
def load_peft_model(peft_model_id):
    """
    ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• LoRA ‡∏î‡πâ‡∏ß‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥
    """
    print(f"--- ‚öôÔ∏è  Starting Model Loading Process ---")
    print(f"üîÑ Attempting to load model from: {peft_model_id}")

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Hugging Face Token
    if os.getenv("HUGGING_FACE_TOKEN") is None:
        print("‚ö†Ô∏è  Warning: HUGGING_FACE_TOKEN environment variable not set.")
        print("   - This may cause issues if the model is private.")

    try:
        config = PeftConfig.from_pretrained(peft_model_id)
        base_model_name = config.base_model_name_or_path
        print(f"‚úÖ Found Base Model: {base_model_name}")

        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
            llm_int8_enable_fp32_cpu_offload=True,
            bnb_4bit_use_double_quant=True
        )

        print("üîÑ Loading Base Model in 4-bit...")
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True,
            offload_folder=OFFLOAD_DIR,
            low_cpu_mem_usage=True
        )
        tokenizer = AutoTokenizer.from_pretrained(base_model_name)
        tokenizer.pad_token = tokenizer.eos_token

        print("üîÑ Loading Adapter (LoRA)...")
        model = PeftModel.from_pretrained(base_model, peft_model_id)

        print("‚úÖ Model and Tokenizer loaded successfully!")
        print("------------------------------------------")
        return model, tokenizer
    except Exception as e:
        print(f"‚ùå CRITICAL ERROR during model loading: {e}")
        import traceback
        traceback.print_exc()
        return None, None


# ==============================================================================
# 3. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö (Inference)
# ==============================================================================
def generate_response(prompt, model, tokenizer):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
    """
    system_prompt = (
        "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ '‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÉ‡∏à‡∏ß‡∏±‡∏¢‡πÄ‡∏£‡∏µ‡∏¢‡∏ô AI' ‡πÄ‡∏õ‡πá‡∏ô AI Chatbot ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡∏≠‡∏Å‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à, ‡πÉ‡∏´‡πâ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÉ‡∏à, ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô"
    )
    chat_prompt = (
        f"<|im_start|>system\n{system_prompt}<|im_end|>\n"
        f"<|im_start|>user\n{prompt}<|im_end|>\n"
        f"<|im_start|>assistant\n"
    )

    inputs = tokenizer(chat_prompt, return_tensors="pt").to(model.device)
    input_len = inputs["input_ids"].shape[1]

    print("ü§ñ Model is generating a response...")
    outputs = model.generate(
        **inputs,
        max_new_tokens=256,
        temperature=0.7,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )
    
    new_tokens = outputs[0, input_len:]
    response = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()
    return response


# ==============================================================================
# 4. Main Execution Loop
# ==============================================================================
if __name__ == "__main__":
    model, tokenizer = load_peft_model(MODEL_PATH)

    if model and tokenizer:
        print("\nüéâ Model is ready! Starting interactive chat session.")
        print("   - Type your message and press Enter.")
        print("   - Type 'quit' or 'exit' to end the session.\n")
        
        while True:
            try:
                user_prompt = input("You: ")
                if user_prompt.lower() in ["quit", "exit"]:
                    print("üëã Exiting chat session. Goodbye!")
                    break
                
                response = generate_response(user_prompt, model, tokenizer)
                print(f"AI: {response}\n")

            except KeyboardInterrupt:
                print("\nüëã Exiting chat session. Goodbye!")
                break
            except Exception as e:
                print(f"üî• An error occurred during inference: {e}")
    else:
        print("\nCould not start chat session because the model failed to load.")
